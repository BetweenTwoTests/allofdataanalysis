[
["index.html", "All of Data Analysis An Expansive Course in Statistical Analysis About", " All of Data Analysis An Expansive Course in Statistical Analysis Leon Kim 2020-05-15 About This is a reproduction of my study notes &amp; exercises from various points of my academic and professional career in statistics. Please excuse the grandiose title. (Fans of statistics should note that the title is tongue-in-cheek reference to this equally grandiose book title ) "],
["1-introduction.html", "Chapter1 Introduction", " Chapter1 Introduction "],
["1-1-hypothesis.html", "1.1 Hypothesis", " 1.1 Hypothesis Some important terms &amp; definitions What is the purpose of statistics? estimate / predict / simplify / organize data Law a rule that implies cause and effect between anything and that applies under same conditions every time. i.e. under some conditions, we can predict what happens all the time but this does not explain “why” the observed cause and effect events happen Model an idealization of the world Hypothesis falsifiable statement regarding the true state of the world true state: assumption that some regularity exists (i.e. law of nature) Hypothesis is supported or refuted by evidence, never proven to be true. Hypothesis is a positive statement of a question. Error is assumed to be present Science proves in negative direction. Our understanding of the world changes by showing that the existing scientific “fact” is “false” and other alternative hypotheses are better candidates to explain the observed events. Let’s call this currently accepted hypothesis as null hypothesis and showing that null hypothesis is “false” as rejecting the null hypothesis. (The words “fact” and “false” are purposely italicized to denote that these terms do not note certainty). You can optionally thing of this as a hypothesis to “null”-ify or a hypothesis of 0 case, null case etc. (one of many words that mean “nothing”). The former “nullify” might make more sense than the “nothing” case if you have no prior exposure to statistics. We will see why the later case is a more popular case of thinking of the null hypothesis. Saying that there are many other alternative hypothesis that could be shown as the better candidate for observed data (1-vs-many case) is more difficult to reason about compared to a simpler, 1-vs-1 case. What if we state our null hypothesis and alternative hypothesis in a way that it is a choice between one or another? i.e. either null hypothesis or alternative hypothesis is a better explanation of the observed data. This is the idea behind how null hypotheses are commonly formulated. Actually, let’s define 3 types of hypothesis (Scientific) Research Hypothesis This is a research question of interested in words, propositions, statements etc. that can be falsified by data. This question does not necessarily have to be quantitative in nature. (Statistical) Null Hypothesis Since we can’t “prove true” in statistics, this is the nullifiable hypothesis that is generally assumed to be true until data shows otherwise. e.g. presumed innocent until guilty. Note that this is can be a re-wording of Research Hypothesis. A Null Hypothesis is generally the default position that there is no relationship between two measured phenomena. Similarly, the default position can be that there are no association among groups of observed values. (Null hypothesis does not necessarily have to be “no” effect. More on this later). We usually denote this with the symbol and equation \\(H_0\\) and formalized as an equation, for example \\(\\text{effect} = 0\\). The word “null” here is taking on both verb and adjective forms in Null Hypothesis. (Statistical) Alternative Hypothesis Since science proves in negative direction, this is the “case B” scenario when the data collected suggests that we should reject the null hypothesis in favor of the alternative. Null hypothesis and alternative hypothesis are mutually exclusive – i.e. if we ever knew what the absolute true state of the world, that absolute truth will be one or the other but not both. We usually denote this with the symbol \\(H_a\\) and formalized as an equation that is negation of the corresponding null hypothesis, for example \\(\\text{effect} \\neq 0\\). The Alternative hypothesis often formulated as the algebraic complement of the null hypothesis. Hence, you will see it stated often as \\(\\neq\\) (compared to null hypothesis of \\(=\\)). This satisfies the mutual exclusivity requirement. But note that mutual exclusivity does not necessarily imply that null and alternative hypothesis taken together have to account for all possibility. Examples to clarify this: Your friend claims that he is very good at guessing the answer to a 5-choice multiple choice questions. If given the chance, he claims that he can guess at every single question without even looking at the question and its choices (which would create a random guessing scenario without him being able to eliminate some of the choices to make “educated guessing”). You are obviously don’t believe that he will be able to get all answers correct, so you give him a test with 10 questions, each with 5 choices. What is the null hypothesis? In this test where you friend randomly guessed at all 10 questions, you would expect him to get approximately 20% correct. This is your default case, driven by what you know about basic probability. So you state that \\(H_0: p = \\frac{1}{5}\\) where \\(p\\) is the percentage score for your test. What is the alternative hypothesis? If we followed the conventional wisdom of stating the alternative as complement of the null, then \\(H_a: p \\neq \\frac{1}{5}\\). You run your experiment and collect the result as \\(p_{friend}\\). You plug these 3 pieces of information to your favorite statistics software cough R cough and gets the result of your hypothesis test using statistics. (You actually would need more than these 3 pieces of information. More on that later too). The software says says “reject the null hypothesis and accept the alternative hypothesis”. You are in shock. Maybe statistic technique you used was wrong? Maybe he was telling the truth and he is a clairvoyant who knows the answers to everything. You start to have existential crisis. But wait, take a look at the \\(H_a\\). It says the test score is not equal to \\(1/5\\). Under this alternative hypothesis, your friend could have gotten all the questions wrong or all the questions right. The former would imply he is not a clairvoyant while the latter does imply he is. Stated in terms of formula, our alternative hypothesis could also be \\(H_a: p &lt; \\frac{1}{5}\\) or \\(H_a: p &gt; \\frac{1}{5}\\). Since we are looking for evidence that your friend is clairvoyant, we should have used the latter alternative hypothesis. You run your statistical software again with the new alternative hypothesis, \\(H_a: p &gt; \\frac{1}{5}\\). The software says “failed to reject the null hypothesis”. Your friend is in an uproar and accurse of being a statistical hack. ~Clearly both of you must enroll in graduate program in statistics to figure out who is right.~ Whether we choose \\(\\neq\\), \\(&lt;\\), or \\(&gt;\\) is related to the statistical technique we use. More on this later, but it should be noted that statistics does not guide the alternative hypothesis. It should be guided by the research hypothesis. Formulating the hypothesis A cursory look would imply that \\(H_0\\) is always “something equals nothing” and \\(H_a\\) is “something equals not-nothing”. But this isn’t always true, as we see in the example. See answer to “How to choose the null and alternative hypothesis?” in Cross Validated Forum for more The way that \\(H_0\\) and \\(H_a\\) are stated in textbooks suggest that you should form the null hypotheis comes first, then the alternative hypothesis. Although this can get you the right answer, the more suitable direction is \\(H_a \\rightarrow H_0\\). The initial starting point of \\(H_a\\) should be guided by your (scientific) Research Hypothesis. The \\(H_0\\) is often the complement of \\(H_a\\) or more often \\(H_0: something = 0\\), but it doesn’t have to be. The “null” in Null Hypothesis doesn’t mean \\(= 0\\). It means the \\(something\\) you wanted is not present, which often is \\(= 0\\) but not always. We often want to see evidence against the null, and this aligns well with the fact that what we want to establish evidence for the Research Hypothesis / Alternative Hypothesis. There are cases where we do not want to reject the null. These scenarios are when we finding modeling something. We will see example of this later on as well. "],
["1-2-examples-of-h-0.html", "1.2 Examples of \\(H_0\\)", " 1.2 Examples of \\(H_0\\) "],
["1-3-research-design.html", "1.3 Research Design", " 1.3 Research Design https://www4.uwsp.edu/psych/mp/c/p300.htm We assume that the universe is orderly, and events have specific causes. We use the scientific method to study the universe The General Procedures for the Scientific Method Ask a question about the world and identify relevant terms needed to ask the question. Operationalize the relevant terms. “Operationalize”: a concept defined by how we measure the terms of interest. Usually involves putting a number on an abstract concept, but doesn’t have to be a number either. Pick a research method Collect &amp; analyze data Research Methods These differ in the kinds of information about behavior they yield, as well as in the types of behavior to which they are best suited for studying. We will look at five different methods. Note that they are not mutually exclusive. Observational AKA systemic observation or naturalistic observation. It is a systematic method for observing behavior as it naturally occurs. This method is characterized by: Unobtrusiveness - subjects are unaware that they are being observed Naturalness - subjects are “at home” and thus assumed to behave as natural as possible Systematic Recording - Behavior is measure and/or counted. For example,  frequencies : how many  duration : how long  latentcies : how long until etc. are recorded for operationally defined behaviors Since researchers are observing and measuring behaviors, there might be differences between observers in agreement. The measure of how reliable operationalized measures are called, unsurprisingly, reliability. Surveys Gathering a information through questionaires, interviews, etc. on a subset of population of interest. This method is characterized by: Sampling a subset of population and measuring these subset (since measuring the whole population is impossible and/or prohibitively costly) Things to consider: Sampling adequacy Case Studies An individual or small group of individuals of interest is studied in detail. These individuals or small group of individualsare called a cohort. This is characterized by two main ways of performing case studies: Retrospective - looks back at past events of the cohorts. Note that we never measured anything in the past. We ask the individuals (and/or people around them, if relevant to the study) about the cohort’s past behaviors. The reliability of this sort of data also depends on the ability for the subject to recall memories. Longitudinal or Proactive - follow events as they occur. We first identify cohorts and continue to study the same cohort for some period of time. This sort of data are very accurate but very costly to do in large scale. Note that many individuals in the cohorts are expected to fall out of the study, so we would have to start with large population. If we are studying a condition that occurs at adulthood and are interested in measuring since birth but we don’t know if a baby would have this condition in the future, then we would have to appropriately have very large cohort to increase our likelihood of capturing individuals of interest by the end of the study. For example schizophrenia that occurs about 1% of the population between age 15 and 35. We want to have 10 subjects with schizophrenia at the end of 35 year study. We should start with 1000 babies and hope for the best that no one drops out (unrealistic, for sure). Experimental Method "],
["2-summary-statistics.html", "Chapter2 Summary Statistics", " Chapter2 Summary Statistics "],
["2-1-frequency.html", "2.1 Frequency", " 2.1 Frequency Table 2.1 shows the number of chocolate chips in a Chips Ahoy cookie measured by 33 graduate students. Each person (Id) measured his/her own cookie twice (Time_1 and Time_2). Prior to collecting this data, the students discussed what ~counts as~ is considered a single chocolate chip. This is actually more difficult than you think because chocolate chips in a cookie comes in different sizes and shapes (individual chip or meleted and merged together as giant glob). Table 2.1: Number of Chocolate Chips Measured by Fall 2019 Class Id Time_1 Time_2 1 19 18 2 17 16 3 9 11 … 31 20 24 32 21 22 33 29 30 Let’s create a frequency table for measurement at time 1 with this dataset. Here are the steps to create a frequency table: Find the highest value and the lowest value of our measured values. In our chocolate chip example, the highest value is 32 and the lowest is 9 for measurements at time 1. Count down from the highest value to the lowest value by unit intervals. Put this sequence in column \\(x\\) of our frequency table in decreasing order. unit: precision width of your measurement, in most cases \\(1\\) since we measure counts by whole numbers. This may seem trivial because what else could it be other than \\(1\\)? Well, this is because we (humans) count numbers by whole numbers, and we are conditioned to think this way. In reality, we made a preliminary decision to what constitutes as 1 chocolate chip versus 2 chocolate chips. In this scenario, it is not possible to measure 1.5 chocolate chips. Note that we are not saying it is impossible to count 1.5 chips – we would just have to define what this is. The 1-vs-2-chips concept is something that we invented for our measurement of the construct of chocolate chip. We as a society “agreed” on a systematic procedural way to count chocolate chips. If we had a machine that measured the magnitude of chocolate chip at \\(0.5\\) intervals, then we would count down by \\(0.5\\) instead. # Step 2 chocochip_unit &lt;- 1 x &lt;- sort(seq(min(df_chips$Time_1), max(df_chips$Time_1), by = chocochip_unit), decreasing = T) df_freq &lt;- tibble(x = x) For each row value of column \\(x\\), count the number of times that value appeared in our measurement. Put this value in column \\(f\\) (for frequency) in our table. x f cf 32 1 33 31 0 32 30 1 32 29 1 31 28 1 30 27 1 29 26 2 28 25 4 26 24 0 22 23 1 22 22 3 21 21 3 18 20 4 15 19 3 11 18 1 8 17 1 7 16 2 6 15 1 4 14 0 3 13 2 3 12 0 1 11 0 1 10 0 1 9 1 1 Note that some frequency counts are 0, for exampe \\(x = 31\\). This is because none of the observers (33 students) did not measure instances of 31 chocolate chips in their cookies. This does not mean Chips Ahoy company does not make cookies with 31 chocolate chips. It just means that we failed to observe such instance due to sampling error. If we had measured all the cookies produced by the company (i.e. population of all cookies) and repeated the measurement experiment, we would know whether there truly wasn’t a single cookie with 31 chips. In our sample of 33 cookies, we don’t know if \\(f = 0\\) if \\(x = 31\\) is due to chance. In the population of all cookies, we would know for a fact that \\(f = 0\\) if \\(x = 31\\) because we have the measurement for the entire population. Also note that saying “This cookie has 1 ‘chocolate chip’” is equivalent to saying “This cookie has somewhere between 0.5 and 1.5 ‘chocolate chip’”. The former is a very strong statement that cannot possibly be true, as we genuinely can’t know what exactly 1 choclate chip is. The latter is a statement that communicates uncertainty due to measurement error of our procedural way to count chocolate chip. This “procedural way to count chocolate chip” is also called operationalization of chocolate chip. To be more generic, “number of chocolate chip” is an example of a construct. Contruct is a broad concept or topic of study interest. Another examples of constructs in non cookie-factory context is “intelligence”. Just like how “number of chocolate chip” can be a difficult thing to define and measure, “intelligence” is difficult to define and measure. Our measurement error comes from various sources, including how we define these constructs and how we operationalize them. # Step 3 # Count # of times each Time_1 value appeared in our x, and put the counts in column f df_freq &lt;- as_tibble( as.data.frame.table( table(x = factor(df_chips$Time_1, levels = df_freq$x)), responseName=&#39;f&#39;) ) We now calculate the cumulative frequency and save to our table as \\(cf\\). To do this, we sum up the frequency \\(f\\) in the ascending ordered values of \\(x\\) (lowest value to highest value). Note that in the R code using library(dplyr), we are ordering x in descending order due to the vocabulary of tidyverse context (In tidyverse table, we are ordering the table rows from top to bottom. Hence, having lowest value of \\(x\\) as first row and highest value of \\(x\\) on the last row is arranging table in descending order). # Step 4 df_freq &lt;- df_freq %&gt;% arrange(desc(x)) %&gt;% # cumulative sum requires ordered x from smallest to highest mutate(cf = cumsum(f)) %&gt;% arrange(x) "]
]
