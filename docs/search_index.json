[
["index.html", "All of Data Analysis An Expansive Course in Statistical Analysis About", " All of Data Analysis An Expansive Course in Statistical Analysis Leon Kim 2020-05-22 About This is a reproduction of my study notes &amp; exercises from various points of my academic and professional career in statistics. Please excuse the grandiose title. (Fans of statistics should note that the title is tongue-in-cheek reference to this equally grandiose book title ) The contents here are not meant to replace existing statistics textbooks. They are meant to fill in some of the gaps, especially in context of psychometrics. "],
["1-introduction.html", "Chapter1 Introduction", " Chapter1 Introduction "],
["1-1-hypothesis.html", "1.1 Hypothesis", " 1.1 Hypothesis Some important terms &amp; definitions What is the purpose of statistics? estimate / predict / simplify / organize data Law a rule that implies cause and effect between anything and that applies under same conditions every time. i.e. under some conditions, we can predict what happens all the time but this does not explain “why” the observed cause and effect events happen Model an idealization of the world Hypothesis falsifiable statement regarding the true state of the world true state: assumption that some regularity exists (i.e. law of nature) Hypothesis is supported or refuted by evidence, never proven to be true. Hypothesis is a positive statement of a question. Error is assumed to be present Science proves in negative direction. Our understanding of the world changes by showing that the existing scientific “fact” is “false” and other alternative hypotheses are better candidates to explain the observed events. Let’s call this currently accepted hypothesis as null hypothesis and showing that null hypothesis is “false” as rejecting the null hypothesis. (The words “fact” and “false” are purposely italicized to denote that these terms do not note certainty). You can optionally thing of this as a hypothesis to “null”-ify or a hypothesis of 0 case, null case etc. (one of many words that mean “nothing”). The former “nullify” might make more sense than the “nothing” case if you have no prior exposure to statistics. We will see why the later case is a more popular case of thinking of the null hypothesis. Saying that there are many other alternative hypothesis that could be shown as the better candidate for observed data (1-vs-many case) is more difficult to reason about compared to a simpler, 1-vs-1 case. What if we state our null hypothesis and alternative hypothesis in a way that it is a choice between one or another? i.e. either null hypothesis or alternative hypothesis is a better explanation of the observed data. This is the idea behind how null hypotheses are commonly formulated. Actually, let’s define 3 types of hypothesis (Scientific) Research Hypothesis This is a research question of interested in words, propositions, statements etc. that can be falsified by data. This question does not necessarily have to be quantitative in nature. (Statistical) Null Hypothesis Since we can’t “prove true” in statistics, this is the nullifiable hypothesis that is generally assumed to be true until data shows otherwise. e.g. presumed innocent until guilty. Note that this is can be a re-wording of Research Hypothesis. A Null Hypothesis is generally the default position that there is no relationship between two measured phenomena. Similarly, the default position can be that there are no association among groups of observed values. (Null hypothesis does not necessarily have to be “no” effect. More on this later). We usually denote this with the symbol and equation \\(H_0\\) and formalized as an equation, for example \\(\\text{effect} = 0\\). The word “null” here is taking on both verb and adjective forms in Null Hypothesis. (Statistical) Alternative Hypothesis Since science proves in negative direction, this is the “case B” scenario when the data collected suggests that we should reject the null hypothesis in favor of the alternative. Null hypothesis and alternative hypothesis are mutually exclusive – i.e. if we ever knew what the absolute true state of the world, that absolute truth will be one or the other but not both. We usually denote this with the symbol \\(H_a\\) and formalized as an equation that is negation of the corresponding null hypothesis, for example \\(\\text{effect} \\neq 0\\). The Alternative hypothesis often formulated as the algebraic complement of the null hypothesis. Hence, you will see it stated often as \\(\\neq\\) (compared to null hypothesis of \\(=\\)). This satisfies the mutual exclusivity requirement. But note that mutual exclusivity does not necessarily imply that null and alternative hypothesis taken together have to account for all possibility. Whether we choose \\(\\neq\\), \\(&lt;\\), or \\(&gt;\\) is related to the statistical technique we use. More on this later, but it should be noted that statistics does not guide the alternative hypothesis. It should be guided by the research hypothesis. Formulating the hypothesis A cursory look would imply that \\(H_0\\) is always “something equals nothing” and \\(H_a\\) is “something equals not-nothing”. But this isn’t always true, as we see in the example. See answer to “How to choose the null and alternative hypothesis?” in Cross Validated Forum for more The way that \\(H_0\\) and \\(H_a\\) are stated in textbooks suggest that you should form the null hypotheis comes first, then the alternative hypothesis. Although this can get you the right answer, the more suitable direction is \\(H_a \\rightarrow H_0\\). The initial starting point of \\(H_a\\) should be guided by your (scientific) Research Hypothesis. The \\(H_0\\) is often the complement of \\(H_a\\) or more often \\(H_0: something = 0\\), but it doesn’t have to be. The “null” in Null Hypothesis doesn’t mean \\(= 0\\). It means the \\(something\\) you wanted is not present, which often is \\(= 0\\) but not always. We often want to see evidence against the null, and this aligns well with the fact that what we want to establish evidence for the Research Hypothesis / Alternative Hypothesis. There are cases where we do not want to reject the null. These scenarios are when we finding modeling something. We will see example of this later on as well. "],
["1-2-examples-of-h-0.html", "1.2 Examples of \\(H_0\\)", " 1.2 Examples of \\(H_0\\) Examples to clarify null hypothesis: Your friend claims that he is very good at guessing the answer to a 5-choice multiple choice questions. If given the chance, he claims that he can guess at every single question without even looking at the question and its choices (which would create a random guessing scenario without him being able to eliminate some of the choices to make “educated guessing”). You are obviously don’t believe that he will be able to get all answers correct, so you give him a test with 10 questions, each with 5 choices. What is the null hypothesis? In this test where you friend randomly guessed at all 10 questions, you would expect him to get approximately 20% correct. This is your default case, driven by what you know about basic probability. So you state that \\(H_0: p = \\frac{1}{5}\\) where \\(p\\) is the percentage score for your test. What is the alternative hypothesis? If we followed the conventional wisdom of stating the alternative as complement of the null, then \\(H_a: p \\neq \\frac{1}{5}\\). You run your experiment and collect the result as \\(p_{friend}\\). You plug these 3 pieces of information to your favorite statistics software cough R cough and gets the result of your hypothesis test using statistics. (You actually would need more than these 3 pieces of information. More on that later too). The software says says “reject the null hypothesis and accept the alternative hypothesis”. You are in shock. Maybe statistic technique you used was wrong? Maybe he was telling the truth and he is a clairvoyant who knows the answers to everything. You start to have existential crisis. But wait, take a look at the \\(H_a\\). It says the test score is not equal to \\(1/5\\). Under this alternative hypothesis, your friend could have gotten all the questions wrong or all the questions right. The former would imply he is not a clairvoyant while the latter does imply he is. Stated in terms of formula, our alternative hypothesis could also be \\(H_a: p &lt; \\frac{1}{5}\\) or \\(H_a: p &gt; \\frac{1}{5}\\). Since we are looking for evidence that your friend is clairvoyant, we should have used the latter alternative hypothesis. You run your statistical software again with the new alternative hypothesis, \\(H_a: p &gt; \\frac{1}{5}\\). The software says “failed to reject the null hypothesis”. Your friend is in an uproar and accurse of being a statistical hack. ~Clearly both of you must enroll in graduate program in statistics to figure out who is right.~ "],
["1-3-research-design.html", "1.3 Research Design", " 1.3 Research Design We assume that the universe is orderly, and events have specific causes. We use the scientific method to study the universe The General Procedures for the Scientific Method Ask a question about the world and identify relevant terms needed to ask the question. Operationalize the relevant terms. “Operationalize”: a concept defined by how we measure the terms of interest. Usually involves putting a number on an abstract concept, but doesn’t have to be a number either. Pick a research method Collect &amp; analyze data Research Methods These differ in the kinds of information about behavior they yield, as well as in the types of behavior to which they are best suited for studying. We will look at five different methods. Note that they are not mutually exclusive. Observational It is a systematic method for observing behavior as it naturally occurs. aka systemic observation or naturalistic observation. This method is characterized by: Unobtrusiveness - subjects are unaware that they are being observed Naturalness - subjects are “at home” and thus assumed to behave as natural as possible Systematic Recording - Behavior is measure and/or counted. For example, frequencies : how many duration : how long latentcies : how long until etc. are recorded for operationally defined behaviors Since researchers are observing and measuring behaviors, there might be differences between observers in agreement. The measure of how reliable operationalized measures are called, unsurprisingly, reliability. Surveys Gathering a information through questionaires, interviews, etc. on a subset of population of interest. This method is characterized by: Sampling a subset of population and measuring these subset (since measuring the whole population is impossible and/or prohibitively costly) Things to consider: Sampling adequacy Case Studies An individual or small group of individuals of interest is studied in detail. These individuals or small group of individualsare called a cohort. This is characterized by two main ways of performing case studies: Retrospective - looks back at past events of the cohorts. Note that we never measured anything in the past. We ask the individuals (and/or people around them, if relevant to the study) about the cohort’s past behaviors. The reliability of this sort of data also depends on the ability for the subject to recall memories. Longitudinal or Proactive - follow events as they occur. We first identify cohorts and continue to study the same cohort for some period of time. This sort of data are very accurate but very costly to do in large scale. Note that many individuals in the cohorts are expected to fall out of the study, so we would have to start with large population. If we are studying a condition that occurs at adulthood and are interested in measuring since birth but we don’t know if a baby would have this condition in the future, then we would have to appropriately have very large cohort to increase our likelihood of capturing individuals of interest by the end of the study. For example schizophrenia that occurs about 1% of the population between age 15 and 35. We want to have 10 subjects with schizophrenia at the end of 35 year study. We should start with 1000 babies and hope for the best that no one drops out (unrealistic, for sure). Experimental Method This is what we most likely are talking about when we say an “experiment to test effect of X on Y”. If randomized control trial (RCT) is used, then this method can provide strong evidence for cause &amp; effect. Independent Variable (IV) - Variable that we can select and manipulate. IV must have at least two levels (categorical) or values (numerical) that IV can take. If IV has only one level or value, then that IV is not useful at all. Dependent Variable (DV) - Variable that we measure and is interested in making inferences about. Sometimes callec criterion variable. Extraneous Variable (EV) - Variable other than IV that can influence the DV. If an EV effects the groups in an experiment (groups determined by the levels of IV, e.g. treatment IV with levels control and drug), then we do not know whether changes in DV are explained by IV or EV. Usually both IVs and EVs effect the variability in DV, and we call such Evs confounding variable. Table 1.1 shows possible effects of EV on DV. Pay close attention to the effect on groups created by IV (e.g. control and drug). Table 1.1: EV &amp; IV on DV Possible Effect on DV Is it a problem? Statistical term EV has no effect on either groups Not a problem ? EV effects all the groups in the same manner Not a problem fixed effect (?) EV effects groups differentially based on which level the subject belongs to Not a problem interaction effect (?) PS Under certain designs, we could say IV effects DV with the word effect as a verb to mean IV “results in or brings about change or consequence” to note cause &amp; effect as apposed to affect which is less stringent and to mean that IV “influences changes in” DV. Nitpicky for sure, but an important distinction. I am assuming that the experimental method would satisfy the condtions for the strong claim of effect. https://www4.uwsp.edu/psych/mp/c/p300.htm "],
["1-4-factors-and-groups.html", "1.4 Factors and Groups", " 1.4 Factors and Groups You often see terms like between-subject and within-subject What are they and how do they relate to experiment design? One IV vs DV Subjects are measured on DV and belongs to one of two groups as denoted by IV level value. In this example, IV, let’s say treatment, has two levels: control and experiment. If there are more than 2 levels (e.g. subject belongs to one of three levels control, experiment 1, or experiment 2), then such experiment would still fall under “one IV” case. In a table representation, the cell values are generally the average \\(\\overline{y}\\) of the measured DV (where \\(y\\) is the DV). The subscripts distinguish which subject (\\(i\\)) and groups (\\(j\\)) the variable \\(y_{ij}\\) refers to. For example, for an experiment with \\(n_1\\) number of subjects in Group 1 (\\(j = 1\\)) and \\(n_2\\) number of subjects in Group 2 (\\(j = 2\\)), there are \\(\\langle y_{1,1}, y_{2,1}, y_{3,1}, ..., y_{n_1, 1}\\rangle\\) measurements associated with subjects in Group 1, and there are \\(\\langle y_{1,2}, y_{2,2}, y_{3,2}, ..., y_{n_2, 2}\\rangle\\) measurements associated with subjects in Group 2. When it is obvious that first subscript (subject index) and second subscript (group index) is obviously distinguishable, the commas between the indices are omitted. When we aggregate over the subjects (as we are doing with averages), we often put a dot \\(\\bullet\\) in place of the \\(i\\) as a placeholder. IV 1 (Treament) Control Experiment DV \\(\\overline{y}_{\\bullet 1}\\) \\(\\overline{y}_{\\bullet 2}\\) You can also be more explicit and say \\(\\overline{y}_{\\bullet,Control}\\), \\(\\overline{y}_{\\bullet,Teatment}\\) instead of \\(\\overline{y}_{\\bullet 1}\\), \\(\\overline{y}_{\\bullet 2}\\), but writing out numbered index instead of full level name is more practical if it is unambiguous that what 1 and 2 refers to. The dot (\\(\\bullet\\)) is also sometimes omitted if it is obvious to do so and if it is acceptable in certain field of study. Two IVs with two levels vs DV When we add another IV, we can apply the same logic as 1 IV case, except the rows in the table now represent IV levels. This is called factorial design. IV 1 (Treament) Control Experiment IV2 (Sex) Male \\(\\overline{y}_{11}\\) \\(\\overline{y}_{12}\\) Female \\(\\overline{y}_{21}\\) \\(\\overline{y}_{22}\\) With 2 IVs, there are three research questions (one for each IV and their interaction): Does IV1 have an effect on the DV? (Not accounting for any other variables, such as IV2) For example if DV is student’s GPA and IV1 is traditional vs novel teaching method, does the teaching method effect GPA? Does IV2 have an effect on the DV? (Not accounting for any other variables, such as IV1). For example if DV is student’s GPA and IV2 is student’s sex, is student sex related to differences in GPA? Note that we do not use the word effect here because student’s sex is demographic information about a student that researcher did not manipulate. Such variable is called ex post facto. Such ex post facto research design and is considered to be a quasi-experimental design. This variable is measuring characteristic or trait about the subject that already existed prior to researcher’s intervention (Treatment), and using groups from such variable to compare the differences in DV is “after the fact” research. Does effect IV1 have on DV depend on the value IV2? Conversely, does effect IV2 have on DV depend on the value of IV1? Factors, Groups, and Between-Subject Design vs Within-Subject Design Factor happens to be a synomym for IV when it takes a fixed number of values (levels). This term has a slightly deeper meaning, but let’s just simplify things and say factor variable is a variable that one of the possible categorical values. We saw that IV with 2 levels can split our subjects into two groups. If the IV had 3 or mor possible levels, then we can obviously split into 3 or more groups. In experimental design. the number of levels do not matter as much as how many IVs we have. Between-Subject Design: Experimental treatments are given to different groups of subject – i.e. no subject can receive more than one treatment at the same time. In Teacher Rating Case Study, there is 1 IV with 2 conditions (instructor review) and 1 DV (professor rating). Students (subjects) were split into one of the two groups by IV / factor. Comparing the mean DV of one group to the mean DV of another group is effectively a comparison between (group of subjects in one condtion) vs (group of subjects in another condition). Two conditions only vary by whether or not IV determined which group the subjects were assigned to. If there were no differences among subjects between groups, then we can interpret the IV directly effecting mean difference. If for some reason one group had particularly generous students who rated professors highly regardless, then we don’t know if the mean difference in professor rating is due to the effect of instructor review condition (IV) or because of generous students. If we randomly assigned which condition a student was exposed to, then we can try to limit accidentally putting all the generous students into one group. However, this does not guarantee that there will not be any differences between groups. There could be another factor unknown to the researcher that is creating differences between groups. Multi-factor Between-Subject Design: same thing as Between-Subject Design with multiple IVs. Within-Subject Design: Unlike Between-Subject Design where a subject is exposed to only one of the levels for a factor (or unique combination of factors in Multi-factor Between Subject Design), each subject is exposed to all levels of a factor. This is done via applying each factor level to each subject in order. Obviously, this design is not possible for all types of factors. Sometimes once a treatment is applied, there’s no reasonable way of “removing” the treatment. Time between each treatment also comes into play here. The order of application of factor level can also become confounding, so we counterbalance by applying factor levels in different order for different subjects. When we consider all the subjects, the number of times a specific order was applied should be same for all possible orders. Because each subject is measured multiple times after different factor levels are applied, this design is often called “repeated measures” design. Within-Subject Design has an advantage that individual differences in subjects’ overall levels of performance are controlled. Meaning the effect of individual subject differences on DV are removed because each subject essentially serves as its own control (e.g. each subject received both control and treatment level. The difference in DV taken after each treatment can be attributed to the most recent factor, ignoring the baseline of other subjects). http://onlinestatbook.com/2/research_design/designs.html "],
["1-5-measurement-scales.html", "1.5 Measurement Scales", " 1.5 Measurement Scales Properties Valid Math Operations Nominal / Categorical None \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Ordinal / Rank Magnitude \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Interval Magnitude \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Interval \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Ratio Magnitude \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Interval \\(=\\), \\(\\neq\\), \\(\\lt\\), \\(\\gt\\) Absolute 0 \\(x_{1} / x_{2} = k\\) Nominal (categorical) scales The categories are mutually exclusive and exhaustive. The categories have no meaning by themselves – the placement of an observation in a particular category simply indicates that it is different from observations in other categories. It does not imply one category is more or less different. Examples: Car model name, eye color Nominal cannot normally distributed. Ordinal (rank) scales Ordinal scales are like nominal scales with additional property of order among the categories included on the scale. Examples: social class, percentile ranks, and Olympic medal placing. With ordinal scales, we can talk about the order of a set of objects, but we cannot say how much bigger one category is to another. Ordinal cannot normally distributed. Interval scales (equi-interval scales) Sometimes called equi-interval scales to distinguish this level of measurement from nominal and ordinal scales. This scale can be added, subtracted, multiplied, and divided without affecting the relative distances among scores.. The intervals between two scores can be determined by arithmetic manipulation of the scores. An important characteristic of interval scales is that the zero point is arbitrary, which means that we cannot meaningfully interpret the size of particular score ratios. As a consequence of being measured on equi-units, it is possible for interval data to be normal under asymptotic assumptions. This is just a fancy way of saying that variables measured in interval scales have central tendency that can be measured by mode, median, or mean. Standard deviations for these variables can also be calculated. Some would argue that centeral tendency for nominal and ordinal data can be calculated via mode. For practical purposes, I think that is valid, but in purist sense, I don’t think central tendency is defined for nominal and ordinal data. Examples: Temperature scale like Celcius, Farenheit 10 Co vs 20 Co and 20 Co vs 30 Co: the degrees are 10 units apart, but just because \\(\\frac{20}{10}=2\\), it doesn’t mean 20 Co is \\(2 \\times 10\\) Co. Your local weather report might say today’s weather (20 Co) is twice as hot as yesterday’s weather (10 Co), but that would be wrong because Celcius scale has no concept of ratio! This has to be with the fact that there is no meaningful \\(0\\) in Celcius. Celcius scale does have 0 to represent freezing point, but this is not a true 0 that represents absence of something, unlike Kelvin scale where 0 is complete absence of molecular motion. Therfore … Not an example: Kelvin Kelvin is not an interval scale. It is a ratio scale because it has properties of interval scale with absolute zero. If two liquids have 10 Kelvin and 20 Kelvin respectively, that does indeed mean the latter is is twice as the former. Ratio scales Ratio scales have all the properties of interval scales with addition of an absolute zero. distance and time measures have genuine zero points. This is what people most generally mean by “numeric” column "],
["2-summary-statistics.html", "Chapter2 Summary Statistics", " Chapter2 Summary Statistics "],
["2-1-limits-rounding.html", "2.1 Limits &amp; Rounding", " 2.1 Limits &amp; Rounding Limits When we measure something with numbers, we are actually speaking in terms of limits based on our unit of measurement. 2.1 shows that when we measure something as \\(x_{i}\\) (for person or observation index, \\(i\\)) equals to some value, then what this really means that “\\(x_{i}\\) was measured via \\(x_{\\bullet, \\text{unit of measurement}}\\), and the value of \\(x_{i}\\) is within the range of \\((x_{i,LL}, x_{i,UL})\\)”. The unit of measurement (\\(x_{\\bullet, \\text{unit of measurement}}\\)) is constant for all observations because we assume that all observations were measured with the same level of precision. This is probably true in most cases. Table 2.1: Example of measurement at 1 unit of measurement Exact Limits \\(x_{i}\\) \\(x_{\\bullet, \\text{unit of measurement}}\\) \\(\\frac{1}{2}\\) of \\(x_{\\bullet, \\text{unit of measurement}}\\) Lower Upper 3 1 0.5 2.5 3.5 7 1 0.5 6.5 7.5 10 1 0.5 9.5 10.5 An example of this is measuring something by counting. You have 3 pencils, 7 apples, and 10 fingers. What you are really doing is saying “I have somewhere between 2.5 and 3.5 pencils, 6.5 and 7.5 apples, and 9.5 and 10.5 fingers”. It seems silly to say you have somewhere between 6.5 and 7.5 apples, but what really is a whole apple? If I have one small apple and one big apple, then you would have said that you have 2 apples, but the sizes of apples are so different! It is just that we as society decided to systematically count objects as whole numbers and not always speak in ranges of possible values because of uncertainty. Some more examples with different number of units. Table 2.2: Example of measurement at 1 unit of measurement Exact Limits \\(x_{i}\\) \\(x_{\\bullet, \\text{unit of measurement}}\\) \\(\\frac{1}{2}\\) of \\(x_{\\bullet, \\text{unit of measurement}}\\) Lower Upper 3.0 0.10 0.050 2.950 3.050 7.5 0.10 0.050 6.950 7.050 10 0.10 0.050 9.950 10.050 3.49 0.01 0.005 3.485 3.495 7.5 0.01 0.005 7.495 7.505 10.61 0.01 0.005 10.605 10.615 Rounding "],
["2-2-frequency.html", "2.2 Frequency", " 2.2 Frequency Table 2.3 shows the number of chocolate chips in a Chips Ahoy cookie measured by 33 graduate students. Each person (Id) measured his/her own cookie twice (Time_1 and Time_2). Prior to collecting this data, the students discussed what ~counts as~ is considered a single chocolate chip. This is actually more difficult than you think because chocolate chips in a cookie comes in different sizes and shapes (individual chip or meleted and merged together as giant glob). Table 2.3: Number of Chocolate Chips Measured by Fall 2019 Class Id Time1 Time2 1 19 18 2 17 16 3 9 11 … 31 20 24 32 21 22 33 29 30 Let’s create a frequency table for measurement at time 1 with this dataset. Here are the steps to create a frequency table: Find the highest value and the lowest value of our measured values. We often refer measurement as a score and use Greek letters to denote variable, like \\(x\\), to represent these scores as a vector of value. In our chocolate chip example, the highest value is 32 and the lowest is 9 for measurements at time 1. We will only look at Time1 value for now Count down from the highest value to the lowest value by measurement unit intervals. Put this sequence in column \\(x\\) of our frequency table in decreasing order. measurement unit: precision width of your measurement, in most cases \\(1\\) since we measure counts by whole numbers. This may seem trivial because what else could it be other than \\(1\\)? Well, this is because we (humans) count numbers by whole numbers, and we are conditioned to think this way. In reality, we made a preliminary decision to what constitutes as 1 chocolate chip versus 2 chocolate chips. In this scenario, it is not possible to measure 1.5 chocolate chips. Note that we are not saying it is impossible to count 1.5 chips – we would just have to define what this is. The 1-vs-2-chips concept is something that we invented for our measurement of the construct of chocolate chip. We as a society “agreed” on a systematic procedural way to count chocolate chips. If we had a agreed on measuring the magnitude of chocolate chip at \\(0.5\\) intervals, then we would count down by \\(0.5\\) instead. # Step 2: Generate dataset over all measurement unit chocochip_unit &lt;- 1 # measurement unit x &lt;- sort(seq(min(df_chips$Time_1), max(df_chips$Time_1), by = chocochip_unit), decreasing = T) df_freq &lt;- tibble(x = x) For each row value of column \\(x\\), count the number of times that value appeared in our measurement. Put this value in column \\(f\\) (for frequency) in our table. Table 2.4: Step 3 - 4 x f cf 32 1 33 31 0 32 30 1 32 29 1 31 28 1 30 27 1 29 26 2 28 25 4 26 24 0 22 23 1 22 22 3 21 21 3 18 20 4 15 19 3 11 18 1 8 17 1 7 16 2 6 15 1 4 14 0 3 13 2 3 12 0 1 11 0 1 10 0 1 9 1 1 Note: Note the 0 in frequency columnwhen x was not measured. Note that some frequency counts are 0, for exampe \\(x = 31\\). This is because none of the observers (33 students) did not measure instances of 31 chocolate chips in their cookies. This does not mean Chips Ahoy company does not make cookies with 31 chocolate chips. It just means that we failed to observe such instance due to sampling error. If we had measured all the cookies produced by the company (i.e. population of all cookies) and repeated the measurement experiment, we would know whether there truly wasn’t a single cookie with 31 chips. In our sample of 33 cookies, we don’t know if \\(f = 0\\) if \\(x = 31\\) is due to chance. In the population of all cookies, we would know for a fact that \\(f = 0\\) if \\(x = 31\\) because we have the measurement for the entire population. Also note that saying “This cookie has 1 ‘chocolate chip’” is equivalent to saying “This cookie has somewhere between 0.5 and 1.5 ‘chocolate chip’”. The former is a very strong statement that cannot possibly be true, as we genuinely can’t know what exactly 1 choclate chip is. The latter is a statement that communicates uncertainty due to measurement error of our procedural way to count chocolate chip. This “procedural way to count chocolate chip” is also called operationalization of chocolate chip. To be more generic, “number of chocolate chip” is an example of a construct. Contruct is a broad concept or topic of study interest. Another examples of constructs in non cookie-factory context is “intelligence”. Just like how “number of chocolate chip” can be a difficult thing to define and measure, “intelligence” is difficult to define and measure. Our measurement error comes from various sources, including how we define these constructs and how we operationalize them. # Step 3: Calculate frequency over min to max of x, for each measurement unit # Count # of times each Time_1 value appeared in our x, and put the counts in column f df_freq &lt;- as_tibble( as.data.frame.table( table(x = factor(df_chips$Time_1, levels = df_freq$x)), responseName=&#39;f&#39;) ) Calculate the cumulative frequency and save to our table as \\(cf\\). To do this, we sum up the frequency \\(f\\) in the ascending ordered values of \\(x\\) (lowest value to highest value). Note that in the R code using library(dplyr), we are ordering x in descending order due to the vocabulary of tidyverse context (In tidyverse table, tibble, we are ordering the table rows from top to bottom. Hence, having lowest value of \\(x\\) as first row and highest value of \\(x\\) on the last row is arranging table in descending order). # Step 4: Calculate cumulative frequency df_freq &lt;- df_freq %&gt;% arrange(desc(x)) %&gt;% # cumulative sum requires ordered x from smallest to highest mutate(cf = cumsum(f)) %&gt;% arrange(x) It looks like we are simply taking a running sum of frequency, but it is very important to understand the meaning of cumulative frequency, especially in terms of limits as per Limits &amp; Rounding. Cumulative frequency by definition is the frequency of scores falling at or below the upper limit of a score. You probably were exposed to cumulative frequency in terms simpler definition of “the frequency by which the observed values X are less than or equal to Xr.” (Wikipedia - Cumulative Frequency Analysis) and/or somehow equated it as discrete version of similar concept from probability distribution as Cumulative Distribution Function (CDF). What you previously know about cumulative frequency is valid, but you should augment that prior knowledge with the concept of measurement and uncertainty. Remember, a score of \\(x_{i}\\) doesn’t actually mean “Observation \\(i\\) scored exactly \\(x_{i}\\).” It is more appropriate to say “Observation \\(i\\) scored somewhere between \\((x_{i} - \\frac{1}{2}\\) of \\(x_{\\bullet, \\text{unit of measurement}})\\) and \\((x_{i} + \\frac{1}{2}\\) of \\(x_{\\bullet, \\text{unit of measurement}})\\).” \\((x_{LL} - x_{UL})\\) x f cf (31.5 - 32.5) 32 1 33 (30.5 - 31.5) 31 0 32 (29.5 - 30.5) 30 1 32 (28.5 - 29.5) 29 1 31 (27.5 - 28.5) 28 1 30 (26.5 - 27.5) 27 1 29 (25.5 - 26.5) 26 2 28 (24.5 - 25.5) 25 4 26 (23.5 - 24.5) 24 0 22 (22.5 - 23.5) 23 1 22 (21.5 - 22.5) 22 3 21 (20.5 - 21.5) 21 3 18 (19.5 - 20.5) 20 4 15 (18.5 - 19.5) 19 3 11 (17.5 - 18.5) 18 1 8 (16.5 - 17.5) 17 1 7 (15.5 - 16.5) 16 2 6 (14.5 - 15.5) 15 1 4 (13.5 - 14.5) 14 0 3 (12.5 - 13.5) 13 2 3 (11.5 - 12.5) 12 0 1 (10.5 - 11.5) 11 0 1 (9.5 - 10.5) 10 0 1 (8.5 - 9.5) 9 1 1 By adding the concept of limits, we can further elaborate on concepts like range that goes beyond what you have probably have learned as “maximum value - minimum value + 1”. What range really is: \\[\\begin{align*} range(X) &amp;= max(x_{UL}) - min(x_{LL}) + class\\ width \\\\ class\\ width &amp;= x_{UL} - x_{LL} \\end{align*}\\] In the most cases where we compute frequency by whole number counts, class width is just equal to the measurement unit which is just 1. That is why the simple rule that you have learned as “max - min + 1” works. It actually is a simplification of this concept. The reason why we use class width instead of simply using “1” will make sense when we create grouped frequency. In short, the range definition with class width is a generalization of “max - min + 1” that works for cases when our unit of measurement is not 1. So — Calculate the proportion (relative frequency) of each \\(x\\) by dividing frequency, \\(f\\), by sum of all frequencies (or equivalently, the max of cumulative frequency). Similarly, calculate the cumulative proportion by dividing cumulative frequency, \\(cf\\), by sum of all frequencies. # Step 5: Calculate relative df_freq &lt;- df_freq %&gt;% mutate(p = f / max(cf), cp = cf / max(cf)) "],
["3-distributions.html", "Chapter3 Distributions", " Chapter3 Distributions "],
["3-1-total-score-difficulty.html", "3.1 Total Score &amp; Difficulty", " 3.1 Total Score &amp; Difficulty Item difficulty \\(p_i\\) is defined as probability of test takers answering the item \\(i\\) correct. This is estimated by \\(p_i = \\frac{\\text{# of test takers who answered correctly}}{\\text{total # of test takers}}\\). Let’s take a look at the relationship between \\(p_i\\) and total score (sum of number of items correct, equally-weighted). In the following simulation, 15-item tests are administered to 1000 people. Different tests have different levels of difficulties for the underlying items. Code to simulate tests taken # For a test with &quot;item_n&quot; number of items, # generate a dataset where &quot;person_n&quot; number of people answered the 15 items. # For all people, each item has &quot;prob&quot; probability of answering correctly. # The vector of &quot;prob&quot; recycles to all the items. # For example, # &quot;prob = 0.5&quot; means all items have 50% chance of correct # &quot;prob = c(0.25, 0.75)&quot; means item 1 has 25% chance, item 2 has 75% chance, # item 3 has 25% chance, item 4 has 75% chance, # ... until you run out of item make_test_score_df &lt;- function(item_n, person_n, prob = 0.5) { if (length(prob) == 1) { df &lt;- sapply(seq_len(item_n), function(i) { rbinom(person_n, size = 1, prob = prob) }) } else { # if prob for all items not specified, recycle to the length of item_n prob &lt;- rep_len(prob, item_n) res &lt;- lapply(seq_along(prob), function(i) { rbinom(person_n, size = 1, prob[i]) }) df &lt;- matrix(unlist(res), ncol = item_n, nrow = person_n, byrow = F) } colnames(df) &lt;- paste0(rep(&quot;Item &quot;, item_n), seq_len(item_n)) df &lt;- df %&gt;% as_tibble() %&gt;% mutate(Person = paste0(rep(&quot;Person &quot;, person_n), seq_len(person_n))) %&gt;% pivot_longer(cols = 1:15, names_to = &quot;Item&quot;) df } # Create Density Plot for simulated dataset make_test_score_plot &lt;- function(df, item_n, person_n, subtitle = &quot;&quot;) { df_Score_by_Person &lt;- df %&gt;% group_by(Person) %&gt;% summarise( Score = sum(value) ) as_tibble( df_freq &lt;- as.data.frame.table( table(Score = factor(df_Score_by_Person$Score, levels = seq(0, item_n, by = 1))), responseName=&#39;Frequency&#39;)) df_freq %&gt;% mutate(RelativeFrequency = Frequency/person_n, Score = as.numeric(as.character(Score))) %&gt;% ggplot(aes(x = Score, y = RelativeFrequency)) + geom_point() + geom_line() + scale_x_continuous(name = &quot;Score&quot;, breaks = seq(0, item_n, by = 1), minor_breaks = seq(0, item_n, by = 1), limits = c(0, item_n)) + stat_function(fun = dnorm, args = list(mean = item_n * 0.5, sd = item_n * 0.5 * (1-0.5)), # expected normal distribution at p = 0.5 col = &quot;red&quot;) + ggtitle(label = paste0(&quot;Distribution of Test Scores for &quot;, person_n, &quot; test takers.&quot;), subtitle = subtitle) + ylab(&quot;Relative Frequency&quot;) } Now we generate 15-item test administered to 1000 people and plot the total score against theoretically normally distributed total score. Note the diffculty \\(p_i\\) (prob in the R code) df_p25 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = 0.25) make_test_score_plot(df_p25, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i], &quot; = &quot;, 0.25,&quot; (hard)&quot;)))) Items have hard difficulty (25% chance of correct), so the total score for test takers are right skewed. df_p50 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = 0.50) make_test_score_plot(df_p50, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i], &quot; = &quot;, 0.50,&quot; (medium)&quot;)))) Items have medium difficulty (50% chance of correct), so the total score for test takers are normally distributed. TODO compare kurtosis? df_p75 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = 0.75) make_test_score_plot(df_p75, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i], &quot; = &quot;, 0.75,&quot; (easy)&quot;)))) Items have east difficulty (75% chance of correct), so the total score for test takers are left skewed. df_p02 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = 0.02) make_test_score_plot(df_p02, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i], &quot; = &quot;, 0.02,&quot; (very hard)&quot;)))) Items have very hard difficulty (2% chance of correct), so the total score for test takers are highly right skewed. This test isn’t useful (Other than maybe identifying that one-in-a-million genius. Regardless, this test is not discriminatory). df_p98 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = 0.98) make_test_score_plot(df_p98, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i], &quot; = &quot;, 0.98,&quot; (very easy)&quot;)))) Items have very easy difficulty (98% chance of correct), so the total score for test takers are highly left skewed. This test isn’t useful because everyone will get perfect or near perfect scores. So what is an ideal test? A good mixture of items varying diffculty around 0.30 - 0.70 (for average of 0.5). But if we are targeting ~0.50 for all items taken together, why not alternate between very hard (\\(p_i = 0.02\\)) and very easy (\\(p_i = 0.98\\)) for the ~0.50? Wouldn’t that lead to the same thing? # Leptokurtic distribution of test scores bc items are too hard or too easy df_p02_p98 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = c(0.02, 0.98)) make_test_score_plot(df_p02_p98, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i],&quot; = 0.02 or &quot;, p[i], &quot;, = 0.98 (very hard or very easy)&quot;)))) The simulation shows that the total score distribution is piling up in the middle, creating highly leptokurtic distribution. To fix this, you have to add more MEDIUM LEVEL items to distribute item’s contribution to the total score to both ends of the total score # To fix, add more moderate difficutly tests df_p02_p50_p98 &lt;- make_test_score_df(item_n = 15, person_n = 1000, prob = c(0.02 , 0.50, 0.50, 0.50, 0.98)) make_test_score_plot(df_p02_p50_p98, item_n = 15, person_n = 1000, subtitle = (expression(paste(&quot;All items: &quot;, p[i],&quot; = {0.02 , 0.50, 0.50, 0.50, 0.98}&quot;)))) At first, you might think “If too many people are scoring ~7.5 (50% of possible max 15), then why should the items be medium diffculty such that 50% of the test takers would answer correctly? Shouldn’t I add more easy / hard items?” Well in the simulation example ??, we took “add more easy / hard items” to the extreme where all items were only very easy or very hard. This distribution happens because score is the total sum of individual item’s correct (1) or incorrect (0). If half of the items were very hard, the sum of those very hard items would be 0 or near 0. On the other hand, the other half of very easy items would have sum of 7 or nearly 8 (or ~8, because we have odd number of items). When you add these halves togehter, the total score hovers around the middle score of ~7.5. "]
]
