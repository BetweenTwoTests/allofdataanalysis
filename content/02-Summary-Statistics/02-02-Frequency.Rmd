## Frequency


```{r 02-02-setup-cache, echo=FALSE, message=FALSE, cache=T}
df_chips <- readRDS("data/chocochip_2019.rds")
```


```{r 02-02-setup, echo=FALSE, message=FALSE}
library(tidyverse)
library(kableExtra)
```


Table \@ref(tab:02-02-chocochip-table) shows the number of chocolate chips in a Chips Ahoy cookie measured by 33 graduate students. Each person (`Id`) measured his/her own cookie twice (`Time_1` and `Time_2`). Prior to collecting this data, the students discussed what ~counts as~ is considered a single chocolate chip. This is actually more difficult than you think because chocolate chips in a cookie comes in different sizes and shapes (individual chip or meleted and merged together as giant glob).


```{r 02-02-chocochip-table, echo=FALSE}
col_width <- "1.5 in"
df_chips_prevew <- bind_rows(
    head(df_chips, n=3),
    tail(df_chips, n=3)
  )
colnames(df_chips_prevew) <- c("Id", "Time~1~", "Time~2~")
kable(
  df_chips_prevew,
  booktabs = TRUE, caption = '<br/>
  Number of Chocolate Chips<br/>
  Measured by Fall 2019 Class'
) %>%
  kable_styling(bootstrap_options = c("striped","condensed"), full_width = F) %>% 
  column_spec(seq_along(df_chips), width = col_width, width_min = col_width, width_max = col_width) %>% 
  pack_rows("...", start_row = 4, end_row = 6, label_row_css = "text-align: center;")
```

Let's create a frequency table for measurement at time 1 with this dataset.

Here are the steps to create a frequency table:

<!-- Step 1 -->

1. Find the highest value and the lowest value of our measured values. We often refer measurement as a _*score*_ and use Greek letters to denote variable, like $x$, to represent these scores as a vector of value.
<br/>
<br/>
In our chocolate chip example, the highest value is 32 and the lowest is 9 for measurements at time 1. We will only look at Time~1~ value for now

---

<!-- Step 2, two column -->

2. Count down from the highest value to the lowest value by [__measurement unit__][Limits & Rounding] intervals. Put this sequence in column $x$ of our frequency table in decreasing order.  
<br/>
__*measurement unit*__: precision width of your measurement, in most cases $1$ since we measure counts by whole numbers. This may seem trivial because what else could it be other than $1$? Well, this is because we (humans) count numbers by whole numbers, and we are conditioned to think this way. In reality, we made a preliminary decision to what constitutes as 1 chocolate chip versus 2 chocolate chips. In this scenario, it is not possible to measure 1.5 chocolate chips. Note that we are not saying it is impossible to count 1.5 chips -- we would just have to define what this is. The 1-vs-2-chips concept is something that we invented for our __measurement__ of the construct of __chocolate chip__. We as a society "agreed" on a systematic procedural way to count chocolate chips.  
<br/>
If we had a agreed on measuring the magnitude of chocolate chip at $0.5$ intervals, then we would count down by $0.5$ instead. 


```{r 02-02-chocochip-freq-table-step2-echo, eval=FALSE, echo=TRUE}
# Step 2: Generate dataset over all measurement unit
chocochip_unit <- 1 # measurement unit
x <- sort(seq(min(df_chips$Time_1), max(df_chips$Time_1), by = chocochip_unit), decreasing = T)
df_freq <- tibble(x = x)
```


---

<!-- Step 3, two column -->

3. For each row value of column $x$, count the number of times that value appeared in our measurement. Put this value in column $f$ (for frequency) in our table.

```{r 02-02-chocochip-freq-table, echo=FALSE}
# Step 2
chocochip_unit <- 1
x <- sort(seq(min(df_chips$Time_1), max(df_chips$Time_1), by = chocochip_unit), decreasing = T)
df_freq <- tibble(x = x)

# Step 3
df_freq <- as_tibble(
  as.data.frame.table(
    table(x = factor(df_chips$Time_1, levels = df_freq$x)), 
    responseName='f')
)

# Step 4
df_freq <- df_freq %>% 
  arrange(desc(x)) %>%
  mutate(cf = cumsum(f)) %>% 
  arrange(x)

col_width <- "1 in"
kable(df_freq,
      caption = '<br/>Step 3 - 4') %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                full_width = F, position = "float_right") %>% 
  column_spec(seq_along(df_freq), width = col_width, width_min = col_width, width_max = col_width) %>% 
  footnote(general = "Note the 0 in frequency column\nwhen `x` was not measured.")
```

Note that some frequency counts are 0, for exampe $x = 31$. This is because none of the observers (33 students) did not measure instances of 31 chocolate chips in their cookies. This does not mean Chips Ahoy company does not make cookies with 31 chocolate chips. It just means that we failed to observe such instance due to __sampling error__. If we had measured all the cookies produced by the company (i.e. __*population*__ of all cookies) and repeated the measurement experiment, we would know whether there truly wasn't a single cookie with 31 chips. In our __*sample*__ of 33 cookies, we don't know if $f = 0$ if $x = 31$ is due to chance. In the __*population*__ of all cookies, we would know for a fact that $f = 0$ if $x = 31$ because we have the measurement for the entire population.  
<br/>
Also note that saying "This cookie has 1 'chocolate chip'" is equivalent to saying "This cookie has somewhere between 0.5 and 1.5 'chocolate chip'". The former is a very strong statement that cannot possibly be true, as we genuinely can't know what __exactly__ 1 choclate chip is. The latter is a statement that communicates uncertainty due to __*measurement error*__ of our procedural way to count chocolate chip.  
<br/>
This "procedural way to count chocolate chip" is also called __*operationalization*__ of chocolate chip. To be more generic, "number of chocolate chip" is an example of a __*construct*__. Contruct is a broad concept or topic of study interest. Another examples of constructs in non cookie-factory context is "intelligence". Just like how "number of chocolate chip" can be a difficult thing to define and measure, "intelligence" is difficult to define and measure. Our measurement error comes from various sources, including how we define these constructs and how we operationalize them.

```{r 02-02-chocochip-freq-table-step3-echo, eval=FALSE, echo=TRUE}
# Step 3: Calculate frequency over min to max of x, for each measurement unit
# Count # of times each Time_1 value appeared in our x, and put the counts in column f
df_freq <- as_tibble(
  as.data.frame.table(
    table(x = factor(df_chips$Time_1, levels = df_freq$x)), 
    responseName='f')
)
```

---

<!-- Step 4-->

4. Calculate the cumulative frequency and save to our table as $cf$. To do this, we sum up the frequency $f$ in the ascending ordered values of $x$ (lowest value to highest value). Note that in the R code using `library(dplyr)`, we are ordering x in __descending__ order due to the vocabulary of `tidyverse` context (In `tidyverse` table, `tibble`, we are ordering the table rows from top to bottom. Hence, having lowest value of $x$ as first row and highest value of $x$ on the last row is arranging table in descending order).

```{r 02-02-chocochip-freq-table-step4-echo, eval=FALSE, echo=TRUE}
# Step 4: Calculate cumulative frequency
df_freq <- df_freq %>% 
  arrange(desc(x)) %>% # cumulative sum requires ordered x from smallest to highest
  mutate(cf = cumsum(f)) %>% 
  arrange(x)
```


It looks like we are simply taking a running sum of frequency, but it is very important to understand the meaning of cumulative frequency, especially in terms of limits as per [Limits & Rounding]. 
<br/>
&nbsp;_Cumulative frequency_ by definition is the frequency of scores falling at or below the upper limit of a score. 
<br/>
You probably were exposed to cumulative frequency in terms simpler definition of "the frequency by which the observed values X are less than or equal to Xr." ([Wikipedia - Cumulative Frequency Analysis](https://en.wikipedia.org/wiki/Cumulative_frequency_analysis)) ~~and/or somehow equated it as discrete version of similar concept from probability distribution as Cumulative Distribution Function (CDF)~~. What you previously know about cumulative frequency is valid, but you should augment that prior knowledge with the concept of measurement and uncertainty. Remember, a score of $x_{i}$ doesn't actually mean "Observation $i$ scored exactly $x_{i}$." It is more appropriate to say "Observation $i$ scored somewhere between $(x_{i} - \frac{1}{2}$ of $x_{\bullet, \text{unit of measurement}})$ and $(x_{i} + \frac{1}{2}$ of $x_{\bullet, \text{unit of measurement}})$."

```{r 02-02-chocohip-freq-table-step5, echo=FALSE}
df_freq_with_limit <- df_freq %>% 
  mutate(`$(x_{LL} - x_{UL})$` = 
           paste0("(", as.numeric(as.character(x)) - chocochip_unit / 2, " - ",
                  as.numeric(levels(x)) + chocochip_unit / 2, ")"))
df_freq_with_limit <- df_freq_with_limit %>% select(c(4, 1:3))
kable(df_freq_with_limit) %>% 
   kable_styling(bootstrap_options = c("striped", "bordered"), full_width = F, position = "float_left")
```

By adding the concept of limits, we can further elaborate on concepts like _range_ that goes beyond what you have probably have learned as "maximum value - minimum value + 1". What range really is:

\begin{align*} 
range(X) &= max(x_{UL}) - min(x_{LL}) + class\ width \\
class\ width &= x_{UL} - x_{LL}
\end{align*} 


In the most cases where we compute frequency by whole number counts, class width is just equal to the measurement unit which is just 1. That is why the simple rule that you have learned as "max - min + 1" works. It actually is a simplification of this concept. The reason why we use class width instead of simply using "1" will make sense when we create _grouped frequency_. In short, the range definition with _class width_ is a generalization of "max - min + 1" that works for cases when our unit of measurement is not 1.
<br/>
So
---

<!-- Step 5-->

5. Calculate the proportion (relative frequency) of each $x$ by dividing frequency, $f$, by sum of all frequencies (or equivalently, the max of cumulative frequency). Similarly, calculate the cumulative proportion by dividing cumulative frequency, $cf$, by sum of all frequencies.


```{r 02-02-chocochip-freq-table-step5-echo, eval=FALSE, echo=TRUE}
# Step 5: Calculate relative
df_freq <- 
  df_freq %>% 
  mutate(p = f / max(cf),
         cp = cf / max(cf))

```



